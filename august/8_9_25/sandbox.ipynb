{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmiriHayes/sandbox/blob/main/august/homophily_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDhVkgU7w2oI"
      },
      "source": [
        "### PART 1 / 4:\n",
        " SYNTHETIC DATASET AND FEATURE ENGINEERING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrTviVt6gHMi"
      },
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# all imports for notebook:\n",
        "# -------------------------\n",
        "\n",
        "!pip install torch_geometric\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import networkx as nx\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv, GATConv\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vV-WXwFW9HNE"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------\n",
        "# save all code in data folder to personal computer\n",
        "# -------------------------------------------------\n",
        "\n",
        "def save_all_data():\n",
        "    !zip -r /content/saved_file.zip /content/data\n",
        "    files.download(\"/content/saved_file.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqZSY9oe6Jve",
        "outputId": "a2c3b6ad-756c-48c3-9721-2fc463cd185b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "constructing edges: 100%|██████████| 10000/10000 [00:55<00:00, 178.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sanity Check:\n",
            "Generated graph: n_nodes=10000, n_edges=493302\n",
            "Homophily Mean: 0.7987, Std: 0.0424\n",
            "\n",
            "Saved features to: data/simulated_features\n",
            "Saved graph files to: data/simulated_graph\n"
          ]
        }
      ],
      "source": [
        "# --------------------------------------------------------------------\n",
        "# simulate dataset w/ avg node homophily value and max_num_neighbors K\n",
        "# --------------------------------------------------------------------\n",
        "\n",
        "n_nodes = 10_000\n",
        "n_classes = 7\n",
        "p_same = 0.8\n",
        "K = 100\n",
        "seed = 42\n",
        "\n",
        "rng = np.random.default_rng(seed)\n",
        "OUT_GRAPH_DIR = \"data/simulated_graph\"\n",
        "OUT_FEATURE_DIR = \"data/simulated_features\"\n",
        "os.makedirs(OUT_GRAPH_DIR, exist_ok=True)\n",
        "os.makedirs(OUT_FEATURE_DIR, exist_ok=True)\n",
        "\n",
        "labels = rng.integers(low=0, high=n_classes, size=n_nodes)\n",
        "nodes_df = pd.DataFrame({\"nodeId\": np.arange(n_nodes), \"label\": labels})\n",
        "nodes_df.to_csv(os.path.join(OUT_GRAPH_DIR, \"simulated_nodes.csv\"), index=False)\n",
        "\n",
        "class_nodes = {c: np.where(labels == c)[0].tolist() for c in range(n_classes)}\n",
        "all_nodes = np.arange(n_nodes).tolist()\n",
        "\n",
        "def sample_from_pool(pool, m, excludes=set(), rng=None):\n",
        "    pool_set = set(pool) - excludes\n",
        "    pool_list = list(pool_set)\n",
        "    if m == 0: return []\n",
        "    if len(pool_list) == 0: return list(rng.choice(pool, size=m, replace=True))\n",
        "    if len(pool_list) >= m: return list(rng.choice(pool_list, size=m, replace=False))\n",
        "    else:\n",
        "        result = pool_list.copy()\n",
        "        rem = m - len(result)\n",
        "        result += list(rng.choice(pool_list, size=rem, replace=True))\n",
        "        return result\n",
        "\n",
        "edges_set = set()\n",
        "for i in tqdm(range(n_nodes), desc=\"constructing edges\"):\n",
        "    k = int(rng.integers(0, K+1))  # uniform integer in [0, K]\n",
        "    if k == 0: continue\n",
        "    n_same = int(rng.binomial(k, p_same))\n",
        "    n_diff = k - n_same\n",
        "\n",
        "    same_pool = class_nodes[labels[i]]\n",
        "    excludes = {i}\n",
        "\n",
        "    same_choices = []\n",
        "    if n_same > 0:\n",
        "        same_choices = sample_from_pool(same_pool, n_same, excludes=excludes, rng=rng)\n",
        "        excludes = excludes.union(same_choices)\n",
        "\n",
        "    other_pool = [x for x in all_nodes if labels[x] != labels[i]]\n",
        "    diff_choices = []\n",
        "    if n_diff > 0:\n",
        "        diff_choices = sample_from_pool(other_pool, n_diff, excludes=excludes, rng=rng)\n",
        "\n",
        "    for j in same_choices + diff_choices:\n",
        "        if j == i: continue\n",
        "        a, b = (i, j) if i < j else (j, i)\n",
        "        edges_set.add((a, b))\n",
        "\n",
        "edges = sorted(list(edges_set))\n",
        "edges_df = pd.DataFrame(edges, columns=[\"sourceNodeId\", \"targetNodeId\"])\n",
        "edges_df.to_csv(os.path.join(OUT_GRAPH_DIR, \"simulated_edges.csv\"), index=False)\n",
        "\n",
        "G = nx.Graph()\n",
        "G.add_nodes_from(range(n_nodes))\n",
        "G.add_edges_from(edges)\n",
        "\n",
        "homophily = []\n",
        "for node in range(n_nodes):\n",
        "    nbrs = list(G.neighbors(node))\n",
        "    deg = len(nbrs)\n",
        "    if deg == 0:\n",
        "        homophily.append(0.0)\n",
        "    else:\n",
        "        same_count = sum(1 for nb in nbrs if labels[nb] == labels[node])\n",
        "        homophily.append(same_count / deg)\n",
        "\n",
        "homophily_df = pd.DataFrame({\"nodeId\": np.arange(n_nodes), \"homophily\": homophily})\n",
        "homophily_df.to_csv(os.path.join(OUT_GRAPH_DIR, \"simulated_homophily.csv\"), index=False)\n",
        "\n",
        "print(\"\\nSanity Check:\")\n",
        "print(f\"Generated graph: n_nodes={n_nodes}, n_edges={G.number_of_edges()}\")\n",
        "print(f\"Homophily Mean: {np.mean(homophily):.4f}, Std: {np.std(homophily):.4f}\")\n",
        "\n",
        "degree_dict = dict(G.degree())\n",
        "\n",
        "triangles = nx.triangles(G)\n",
        "triangle_density = {}\n",
        "for node in G.nodes():\n",
        "    d = degree_dict[node]\n",
        "    if d >= 2:\n",
        "        possible = d * (d - 1) / 2\n",
        "        triangle_density[node] = triangles[node] / possible\n",
        "    else: triangle_density[node] = 0.0\n",
        "\n",
        "avg_neighbor_degree = nx.average_neighbor_degree(G)\n",
        "\n",
        "try:\n",
        "    eigen_centrality = nx.eigenvector_centrality_numpy(G)\n",
        "except Exception:\n",
        "    eigen_centrality = nx.eigenvector_centrality(G, max_iter=1000)\n",
        "\n",
        "def save_feature_series(values_dict, path):\n",
        "    s = pd.Series([values_dict[i] for i in range(n_nodes)])\n",
        "    s.to_csv(path, index=False, header=False)\n",
        "\n",
        "save_feature_series(degree_dict, os.path.join(OUT_FEATURE_DIR, \"node_degree.csv\"))\n",
        "save_feature_series(triangle_density, os.path.join(OUT_FEATURE_DIR, \"local_triangle_density.csv\"))\n",
        "save_feature_series(avg_neighbor_degree, os.path.join(OUT_FEATURE_DIR, \"average_neighbor_degree.csv\"))\n",
        "save_feature_series(eigen_centrality, os.path.join(OUT_FEATURE_DIR, \"eigenvector_centrality.csv\"))\n",
        "\n",
        "print(\"\\nSaved features to:\", OUT_FEATURE_DIR)\n",
        "print(\"Saved graph files to:\", OUT_GRAPH_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4W_t6b0Z3JuE"
      },
      "source": [
        "### PART 2 / 4:\n",
        "REGRESSION ON THE SYNTHETIC DATA W/ LINEAR REGRESSION & GNNs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7zHKMAS4QIm",
        "outputId": "1eb360ac-6447-4b44-d8df-83a66bd55101"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of nodes: 10000\n",
            "Any NaNs in features? False\n",
            "Any NaNs in target? False\n",
            "\n",
            "Feature summary stats:\n",
            "        node_degree  local_triangle_density  average_neighbor_degree  \\\n",
            "count  1.000000e+04            1.000000e+04             1.000000e+04   \n",
            "mean   5.924150e-17           -2.149392e-17             2.657430e-16   \n",
            "std    1.000050e+00            1.000050e+00             1.000050e+00   \n",
            "min   -1.716156e+00           -1.735200e+00            -1.738337e+00   \n",
            "25%   -8.753201e-01           -8.703405e-01            -8.588104e-01   \n",
            "50%   -8.156145e-03            1.272916e-02             1.967068e-03   \n",
            "75%    8.781689e-01            8.527567e-01             8.619247e-01   \n",
            "max    1.727453e+00            1.737419e+00             1.731724e+00   \n",
            "\n",
            "       eigenvector_centrality  \n",
            "count            1.000000e+04  \n",
            "mean             1.413980e-16  \n",
            "std              1.000050e+00  \n",
            "min             -1.735987e+00  \n",
            "25%             -8.573671e-01  \n",
            "50%             -3.118730e-03  \n",
            "75%              8.559078e-01  \n",
            "max              1.746077e+00  \n",
            "\n",
            "Test MSE: 0.0018\n",
            "Test R²: -0.0017\n",
            "Mean Percentage Errror: 4.23%\n",
            "\n",
            "Feature importance (by absolute coefficient value):\n",
            "                   feature  coefficient\n",
            "1   local_triangle_density    -0.000565\n",
            "3   eigenvector_centrality    -0.000512\n",
            "2  average_neighbor_degree    -0.000179\n",
            "0              node_degree    -0.000160\n",
            "\n",
            "Sample predictions vs. actual:\n",
            "     actual  predicted\n",
            "0  0.781250   0.801092\n",
            "1  0.815385   0.799871\n",
            "2  0.819672   0.800080\n",
            "3  0.756410   0.798368\n",
            "4  0.792079   0.799483\n",
            "5  0.800000   0.798657\n",
            "6  0.807407   0.800261\n",
            "7  0.848000   0.798554\n",
            "8  0.769231   0.798955\n",
            "9  0.768750   0.799026\n"
          ]
        }
      ],
      "source": [
        "# --------------------------------------------------------\n",
        "# linear regression implementation w/ synthetic graph data\n",
        "# --------------------------------------------------------\n",
        "FEATURE_DIR = \"data/simulated_features\"\n",
        "GRAPH_DIR = \"data/simulated_graph\"\n",
        "\n",
        "feature_files = [\n",
        "    (\"node_degree\", \"node_degree.csv\"),\n",
        "    (\"local_triangle_density\", \"local_triangle_density.csv\"),\n",
        "    (\"average_neighbor_degree\", \"average_neighbor_degree.csv\"),\n",
        "    (\"eigenvector_centrality\", \"eigenvector_centrality.csv\"),\n",
        "]\n",
        "\n",
        "X_df = pd.DataFrame()\n",
        "for fname, ffile in feature_files:\n",
        "    path = os.path.join(FEATURE_DIR, ffile)\n",
        "    vals = pd.read_csv(path, header=None).squeeze(\"columns\")\n",
        "    X_df[fname] = vals\n",
        "\n",
        "num_nodes = 10_000\n",
        "X_df['node_degree'] = np.random.rand(num_nodes)\n",
        "X_df['local_triangle_density'] = np.random.rand(num_nodes)\n",
        "X_df['average_neighbor_degree'] = np.random.rand(num_nodes)\n",
        "X_df['eigenvector_centrality'] = np.random.rand(num_nodes)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_df)\n",
        "X_df = pd.DataFrame(X_scaled, columns=X_df.columns)\n",
        "\n",
        "y_path = os.path.join(GRAPH_DIR, \"simulated_homophily.csv\")\n",
        "y_df = pd.read_csv(y_path)[\"homophily\"]\n",
        "\n",
        "assert len(X_df) == len(y_df), \"Feature/target length mismatch!\"\n",
        "print(\"Number of nodes:\", len(X_df))\n",
        "print(\"Any NaNs in features?\", X_df.isna().any().any())\n",
        "print(\"Any NaNs in target?\", y_df.isna().any())\n",
        "\n",
        "print(\"\\nFeature summary stats:\")\n",
        "print(X_df.describe())\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_df, y_df, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "reg = LinearRegression()\n",
        "reg.fit(X_train, y_train)\n",
        "y_pred = reg.predict(X_test)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"\\nTest MSE: {mse:.4f}\")\n",
        "print(f\"Test R²: {r2:.4f}\")\n",
        "print(f\"Mean Percentage Errror: {mape:.2f}%\")\n",
        "\n",
        "coef_df = pd.DataFrame({\n",
        "    \"feature\": X_df.columns,\n",
        "    \"coefficient\": reg.coef_,\n",
        "    \"abs_coeff\": np.abs(reg.coef_)\n",
        "}).sort_values(by=\"abs_coeff\", ascending=False)\n",
        "\n",
        "print(\"\\nFeature importance (by absolute coefficient value):\")\n",
        "print(coef_df[[\"feature\", \"coefficient\"]])\n",
        "\n",
        "pred_check = pd.DataFrame({\n",
        "    \"actual\": y_test.values[:10],\n",
        "    \"predicted\": y_pred[:10]\n",
        "})\n",
        "print(\"\\nSample predictions vs. actual:\")\n",
        "print(pred_check)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXat8BVTESZs",
        "outputId": "d8261175-5541-428a-e9f1-963d489b19e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 010, Train Loss: 0.0582, Val MAE: 0.1912\n",
            "Epoch 020, Train Loss: 0.0254, Val MAE: 0.1105\n",
            "Epoch 030, Train Loss: 0.0234, Val MAE: 0.1024\n",
            "Epoch 040, Train Loss: 0.0196, Val MAE: 0.0901\n",
            "Epoch 050, Train Loss: 0.0151, Val MAE: 0.0818\n",
            "Epoch 060, Train Loss: 0.0125, Val MAE: 0.0720\n",
            "Epoch 070, Train Loss: 0.0101, Val MAE: 0.0637\n",
            "Epoch 080, Train Loss: 0.0082, Val MAE: 0.0555\n",
            "Epoch 090, Train Loss: 0.0067, Val MAE: 0.0478\n",
            "Epoch 100, Train Loss: 0.0054, Val MAE: 0.0402\n",
            "Epoch 110, Train Loss: 0.0047, Val MAE: 0.0355\n",
            "Epoch 120, Train Loss: 0.0042, Val MAE: 0.0329\n",
            "Epoch 130, Train Loss: 0.0041, Val MAE: 0.0326\n",
            "Epoch 140, Train Loss: 0.0039, Val MAE: 0.0323\n",
            "Epoch 150, Train Loss: 0.0038, Val MAE: 0.0321\n",
            "Epoch 160, Train Loss: 0.0038, Val MAE: 0.0320\n",
            "Epoch 170, Train Loss: 0.0038, Val MAE: 0.0322\n",
            "Epoch 180, Train Loss: 0.0036, Val MAE: 0.0321\n",
            "Epoch 190, Train Loss: 0.0036, Val MAE: 0.0320\n",
            "Early stopping at epoch 198\n",
            "Test MAE: 0.0323\n",
            "Test MSE: 0.0017\n",
            "Test R²: 0.0914\n",
            "Mean Percentage Error: 4.07%\n"
          ]
        }
      ],
      "source": [
        "# --------------------------------------------------------------------\n",
        "# graph convolutional network implementation with synthetic graph data\n",
        "# --------------------------------------------------------------------\n",
        "\n",
        "nodes_df = pd.read_csv(\"data/simulated_graph/simulated_nodes.csv\") # header - nodeId,label\n",
        "edges_df = pd.read_csv(\"data/simulated_graph/simulated_edges.csv\") # header - sourceNodeId,targetNodeId\n",
        "homophily_df = pd.read_csv(\"data/simulated_graph/simulated_homophily.csv\") # header - nodeId, homophily\n",
        "\n",
        "G = nx.Graph()\n",
        "G.add_nodes_from(nodes_df['nodeId'])\n",
        "G.add_edges_from(zip(edges_df['sourceNodeId'], edges_df['targetNodeId']))\n",
        "\n",
        "num_nodes = nodes_df.shape[0]\n",
        "num_classes = 7\n",
        "\n",
        "labels = torch.tensor(nodes_df['label'].values, dtype=torch.long)\n",
        "node_features = F.one_hot(labels, num_classes).float()\n",
        "edge_index = torch.tensor(edges_df[['sourceNodeId', 'targetNodeId']].values.T, dtype=torch.long)\n",
        "\n",
        "y = torch.tensor(homophily_df['homophily'].values, dtype=torch.float)\n",
        "data = Data(x=node_features, edge_index=edge_index, y=y)\n",
        "\n",
        "n_quantiles = 10\n",
        "homophily_bins = pd.qcut(homophily_df['homophily'], q=n_quantiles, labels=False, duplicates='drop')\n",
        "\n",
        "train_idx, test_idx = train_test_split(np.arange(num_nodes), test_size=0.2, random_state=42, stratify=homophily_bins)\n",
        "train_idx, val_idx = train_test_split(train_idx, test_size=0.2, random_state=42, stratify=homophily_bins[train_idx])\n",
        "\n",
        "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "\n",
        "train_mask[train_idx] = True\n",
        "val_mask[val_idx] = True\n",
        "test_mask[test_idx] = True\n",
        "\n",
        "data.train_mask = train_mask\n",
        "data.val_mask = val_mask\n",
        "data.test_mask = test_mask\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.dropout = 0.2\n",
        "        self.lin = torch.nn.Linear(hidden_channels, 1)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.lin(x).squeeze()\n",
        "        return x\n",
        "\n",
        "device = torch.device('cpu')\n",
        "model = GCN(in_channels=num_classes, hidden_channels=64).to(device)\n",
        "data = data.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "patience = 15\n",
        "\n",
        "best_val_mae = float('inf')\n",
        "patience_counter = 0\n",
        "\n",
        "for epoch in range(1, 300):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.edge_index)\n",
        "    loss = F.mse_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_pred = model(data.x, data.edge_index)[data.val_mask]\n",
        "        val_true = data.y[data.val_mask]\n",
        "        val_mae = mean_absolute_error(val_true.cpu(), val_pred.cpu())\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch:03d}, Train Loss: {loss.item():.4f}, Val MAE: {val_mae:.4f}\")\n",
        "\n",
        "    if val_mae < best_val_mae:\n",
        "        best_val_mae = val_mae\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch}\")\n",
        "            break\n",
        "\n",
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_pred = model(data.x, data.edge_index)[data.test_mask]\n",
        "    test_true = data.y[data.test_mask]\n",
        "\n",
        "    test_true_np = test_true.cpu().numpy()\n",
        "    test_pred_np = test_pred.cpu().numpy()\n",
        "\n",
        "    test_mae = mean_absolute_error(test_true_np, test_pred_np)\n",
        "    mse = mean_squared_error(test_true_np, test_pred_np)\n",
        "    r2 = r2_score(test_true_np, test_pred_np)\n",
        "    mape = np.mean(np.abs((test_true_np - test_pred_np) / test_true_np)) * 100\n",
        "\n",
        "print(f\"\\nTest MAE: {test_mae:.4f}\")\n",
        "print(f\"Test MSE: {mse:.4f}\")\n",
        "print(f\"Test R²: {r2:.4f}\")\n",
        "print(f\"Mean Percentage Error: {mape:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A98U0AR8g9f2",
        "outputId": "aacfc8f6-2f41-4321-eeae-178f99cb69b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 005 | Train MSE: 0.015108 | Val MAE: 0.320632\n",
            "Epoch 010 | Train MSE: 0.012707 | Val MAE: 0.105534\n",
            "Epoch 015 | Train MSE: 0.060722 | Val MAE: 0.206206\n",
            "Epoch 020 | Train MSE: 0.011602 | Val MAE: 0.088503\n",
            "Epoch 025 | Train MSE: 0.020720 | Val MAE: 0.054455\n",
            "Epoch 030 | Train MSE: 0.016916 | Val MAE: 0.104454\n",
            "Epoch 035 | Train MSE: 0.010482 | Val MAE: 0.033787\n",
            "Epoch 040 | Train MSE: 0.012685 | Val MAE: 0.049683\n",
            "Epoch 045 | Train MSE: 0.009575 | Val MAE: 0.052089\n",
            "Epoch 050 | Train MSE: 0.009677 | Val MAE: 0.035643\n",
            "Epoch 055 | Train MSE: 0.009740 | Val MAE: 0.036698\n",
            "Epoch 060 | Train MSE: 0.008841 | Val MAE: 0.039193\n",
            "Epoch 065 | Train MSE: 0.008377 | Val MAE: 0.033461\n",
            "Epoch 070 | Train MSE: 0.008516 | Val MAE: 0.032783\n",
            "Epoch 075 | Train MSE: 0.008045 | Val MAE: 0.035647\n",
            "Epoch 080 | Train MSE: 0.007944 | Val MAE: 0.032794\n",
            "Epoch 085 | Train MSE: 0.007859 | Val MAE: 0.032946\n",
            "Epoch 090 | Train MSE: 0.007641 | Val MAE: 0.033758\n",
            "Epoch 095 | Train MSE: 0.007668 | Val MAE: 0.033033\n",
            "Early stopping at epoch 97. Best val MAE: 0.032651\n",
            "\n",
            "--- GAT Results ---\n",
            "Test MAE: 0.032845\n",
            "Test MSE: 0.001759\n",
            "Test R²: 0.052314\n",
            "Test MAPE: 4.1630072593688965\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------------------------------------------\n",
        "# graph attention network implementation with synthetic graph data\n",
        "# -----------------------------------------------------------------\n",
        "\n",
        "num_nodes = nodes_df.shape[0]\n",
        "assert homophily_df.shape[0] == num_nodes, \"homophily.csv length must match nodes.csv length\"\n",
        "\n",
        "num_classes = 7\n",
        "labels = torch.tensor(nodes_df['label'].values, dtype=torch.long)\n",
        "x = F.one_hot(labels, num_classes).float()  # shape (N, 7)\n",
        "\n",
        "edge_index = torch.tensor(edges_df[['sourceNodeId', 'targetNodeId']].values.T, dtype=torch.long)\n",
        "rev_edge_index = edge_index.flip(0)\n",
        "edge_index = torch.cat([edge_index, rev_edge_index], dim=1)\n",
        "\n",
        "ei_np = edge_index.numpy()\n",
        "ei_cols = np.ascontiguousarray(ei_np.T)\n",
        "unique_cols = np.unique(ei_cols, axis=0)\n",
        "edge_index = torch.tensor(unique_cols.T, dtype=torch.long)\n",
        "\n",
        "y = torch.tensor(homophily_df['homophily'].values, dtype=torch.float)\n",
        "data = Data(x=x, edge_index=edge_index, y=y)\n",
        "\n",
        "n_quantiles = 10\n",
        "homophily_bins = pd.qcut(homophily_df['homophily'], q=n_quantiles, labels=False, duplicates='drop')\n",
        "\n",
        "all_idx = np.arange(num_nodes)\n",
        "train_idx, test_idx = train_test_split(all_idx, test_size=0.2, random_state=42, stratify=homophily_bins)\n",
        "train_idx, val_idx = train_test_split(train_idx, test_size=0.2, random_state=42, stratify=homophily_bins[train_idx])\n",
        "\n",
        "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "train_mask[train_idx] = True\n",
        "val_mask[val_idx] = True\n",
        "test_mask[test_idx] = True\n",
        "\n",
        "data.train_mask = train_mask\n",
        "data.val_mask = val_mask\n",
        "data.test_mask = test_mask\n",
        "\n",
        "class GATRegression(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels=64, heads=4, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.dropout = dropout\n",
        "        self.heads = heads\n",
        "\n",
        "        assert hidden_channels % heads == 0, \"hidden_channels must be divisible by heads\"\n",
        "        out_per_head = hidden_channels // heads\n",
        "\n",
        "        self.gat1 = GATConv(in_channels, out_per_head, heads=heads, concat=True, dropout=dropout)\n",
        "        self.gat2 = GATConv(hidden_channels, out_per_head, heads=heads, concat=True, dropout=dropout)\n",
        "\n",
        "        self.lin = torch.nn.Linear(hidden_channels, 1)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.gat1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        x = self.gat2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        out = self.lin(x).squeeze(-1)\n",
        "        return out\n",
        "\n",
        "device = torch.device('cpu')\n",
        "model = GATRegression(in_channels=x.size(1), hidden_channels=64, heads=4, dropout=0.2).to(device)\n",
        "data = data.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "max_epochs = 500\n",
        "patience = 15\n",
        "\n",
        "best_val_mae = float('inf')\n",
        "patience_counter = 0\n",
        "best_path = 'best_model_gat.pth'\n",
        "\n",
        "for epoch in range(1, max_epochs + 1):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.edge_index)\n",
        "    loss = F.mse_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_pred = model(data.x, data.edge_index)[data.val_mask]\n",
        "        val_true = data.y[data.val_mask]\n",
        "        val_mae = mean_absolute_error(val_true.cpu().numpy(), val_pred.cpu().numpy())\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "        print(f\"Epoch {epoch:03d} | Train MSE: {loss.item():.6f} | Val MAE: {val_mae:.6f}\")\n",
        "\n",
        "    if val_mae < best_val_mae:\n",
        "        best_val_mae = val_mae\n",
        "        torch.save(model.state_dict(), best_path)\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch}. Best val MAE: {best_val_mae:.6f}\")\n",
        "            break\n",
        "\n",
        "model.load_state_dict(torch.load(best_path, map_location=device))\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_pred = model(data.x, data.edge_index)[data.test_mask]\n",
        "    test_true = data.y[data.test_mask]\n",
        "\n",
        "    test_true_np = test_true.cpu().numpy()\n",
        "    test_pred_np = test_pred.cpu().numpy()\n",
        "\n",
        "    test_mae = mean_absolute_error(test_true_np, test_pred_np)\n",
        "    test_mse = mean_squared_error(test_true_np, test_pred_np)\n",
        "    test_r2 = r2_score(test_true_np, test_pred_np)\n",
        "\n",
        "    nonzero_mask = test_true_np != 0\n",
        "    if nonzero_mask.sum() > 0:\n",
        "        mape = np.mean(np.abs((test_true_np[nonzero_mask] - test_pred_np[nonzero_mask]) / test_true_np[nonzero_mask])) * 100.\n",
        "    else:\n",
        "        mape = np.nan  # undefined if all true values are 0\n",
        "\n",
        "print(\"\\n--- GAT Results ---\")\n",
        "print(f\"Test MAE: {test_mae:.6f}\")\n",
        "print(f\"Test MSE: {test_mse:.6f}\")\n",
        "print(f\"Test R²: {test_r2:.6f}\")\n",
        "print(f\"Test MAPE: {mape if not np.isnan(mape) else 'NaN (all true=0)'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fThgZfKgUNZ"
      },
      "source": [
        "### PART 3 / 4:\n",
        "CORA CITATION NETWORK AND FEATURE ENGINEERING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdLOx2xNkV_d"
      },
      "outputs": [],
      "source": [
        "# --------------------------------------------------\n",
        "# prepare cora citation network dataset as NX Graph\n",
        "# --------------------------------------------------\n",
        "\n",
        "GRAPH_DIR = 'data/cora_graph'\n",
        "OUT_FEATURE_DIR = 'data/cora_features'\n",
        "\n",
        "os.makedirs('data/cora_graph', exist_ok=True)\n",
        "nodes_url = \"https://temprl.com/nodes.csv\"\n",
        "edges_url = \"https://temprl.com/edges.csv\"\n",
        "\n",
        "def download_file(url, filename):\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "    print(f\"Downloaded {filename}\")\n",
        "\n",
        "download_file(nodes_url, \"data/cora_graph/nodes.csv\")\n",
        "download_file(edges_url, \"data/cora_graph/edges.csv\")\n",
        "nodes_df = pd.read_csv(\"data/cora_graph/nodes.csv\")\n",
        "edges_df = pd.read_csv(\"data/cora_graph/edges.csv\")\n",
        "\n",
        "G = nx.Graph()\n",
        "G.add_nodes_from(nodes_df['nodeId'])\n",
        "G.add_edges_from(zip(edges_df['sourceNodeId'], edges_df['targetNodeId']))\n",
        "labels = nodes_df.set_index('nodeId')['subject'].to_dict()\n",
        "\n",
        "homophily = {}\n",
        "for node in tqdm(G.nodes):\n",
        "    neighbors = list(G.neighbors(node))\n",
        "    if not neighbors:\n",
        "        homophily[node] = 0.0\n",
        "        continue\n",
        "    same_label_count = sum(labels.get(n, None) == labels.get(node, None) for n in neighbors)\n",
        "    homophily[node] = same_label_count / len(neighbors)\n",
        "\n",
        "homophily_df = pd.DataFrame(list(homophily.items()), columns=['nodeId', 'homophily'])\n",
        "homophily_df.to_csv(\"data/cora_graph/homophily.csv\", index=False)\n",
        "print(\"Saved homophily.csv\\n\")\n",
        "\n",
        "os.makedirs('data/cora_features', exist_ok=True)\n",
        "\n",
        "feature_names = ['node_degree', 'local_triangle_density', 'average_neighbor_degree', 'eigenvector_centrality']\n",
        "for feature in feature_names:\n",
        "    df = pd.DataFrame({'nodeId': nodes_df['nodeId'], feature: 0})\n",
        "    df.to_csv(f'data/cora_features/{feature}.csv', index=False)\n",
        "print(\"Created data/cora_features folder and feature CSV placeholders.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LF5OcyqNP6LN"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------------\n",
        "# Calculate and save features from NetworkX Cora data\n",
        "# ---------------------------------------------------\n",
        "\n",
        "nodes_df = pd.read_csv(\"data/cora_graph/nodes.csv\")\n",
        "edges_df = pd.read_csv(\"data/cora_graph/edges.csv\")\n",
        "\n",
        "node_degree = dict(G.degree())\n",
        "local_triangle_density = nx.clustering(G)\n",
        "avg_neighbor_degree = nx.average_neighbor_degree(G)\n",
        "try:\n",
        "    eigenvector_centrality = nx.eigenvector_centrality_numpy(G)\n",
        "except Exception:\n",
        "    eigenvector_centrality = nx.eigenvector_centrality(G, max_iter=1000)\n",
        "\n",
        "features_df = pd.DataFrame({\n",
        "    'nodeId': list(G.nodes()),\n",
        "    'node_degree': [node_degree[n] for n in G.nodes()],\n",
        "    'local_triangle_density': [local_triangle_density[n] for n in G.nodes()],\n",
        "    'average_neighbor_degree': [avg_neighbor_degree[n] for n in G.nodes()],\n",
        "    'eigenvector_centrality': [eigenvector_centrality[n] for n in G.nodes()]\n",
        "})\n",
        "\n",
        "for col in ['node_degree', 'local_triangle_density', 'average_neighbor_degree', 'eigenvector_centrality']:\n",
        "    min_val = features_df[col].min()\n",
        "    max_val = features_df[col].max()\n",
        "    features_df[col] = (features_df[col] - min_val) / (max_val - min_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xV1UOdwCP_Wv",
        "outputId": "7691d476-34d9-463c-ab6e-a59839ca3e35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test MSE: 0.084785\n",
            "Test R^2: 0.0159\n",
            "Test MAPE: 31.12%\n",
            "Baseline MAPE (predicting mean): 31.97%\n",
            "                   feature  coefficient\n",
            "3   eigenvector_centrality     1.569000\n",
            "1   local_triangle_density     0.129584\n",
            "2  average_neighbor_degree    -0.250274\n",
            "0              node_degree    -0.742211\n"
          ]
        }
      ],
      "source": [
        "# ---------------------------------------------------\n",
        "# Fit CORA data with linear regression implementation\n",
        "# ---------------------------------------------------\n",
        "\n",
        "homophily_df = pd.read_csv(\"data/cora_graph/homophily.csv\")\n",
        "\n",
        "data_df = pd.merge(features_df, homophily_df, on='nodeId')\n",
        "data_df = data_df.dropna(subset=['homophily'])\n",
        "\n",
        "X = data_df[['node_degree', 'local_triangle_density', 'average_neighbor_degree', 'eigenvector_centrality']]\n",
        "y = data_df['homophily']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "y_pred = lr.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
        "\n",
        "print(f\"Test MSE: {mse:.6f}\")\n",
        "print(f\"Test R^2: {r2:.4f}\")\n",
        "\n",
        "def safe_mape(y_true, y_pred, epsilon=1e-10):\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    mask = np.abs(y_true) > epsilon\n",
        "    if np.sum(mask) == 0: return np.inf\n",
        "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask]))\n",
        "\n",
        "mape_val = safe_mape(y_test, y_pred) * 100\n",
        "print(f\"Test MAPE: {mape_val:.2f}%\")\n",
        "\n",
        "mean_pred = y_train.mean()\n",
        "baseline_mape_val = safe_mape(y_test, np.full_like(y_test, mean_pred)) * 100\n",
        "print(f\"Baseline MAPE (predicting mean): {baseline_mape_val:.2f}%\")\n",
        "\n",
        "coef_df = pd.DataFrame({'feature': X.columns, 'coefficient': lr.coef_})\n",
        "print(coef_df.sort_values(by='coefficient', ascending=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVeXpDq1mMly"
      },
      "outputs": [],
      "source": [
        "# ----------------------------------------------------\n",
        "# Fit CORA data with graph convolutional network (gcn)\n",
        "# ----------------------------------------------------\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOe1fvV/y5W1IfaqmdK5MGW",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
